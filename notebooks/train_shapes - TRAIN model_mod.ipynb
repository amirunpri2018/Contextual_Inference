{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Mask R-CNN - Train modified model on old shapes dataset\n",
    "\n",
    "### the modified model does not include any mask related heads or losses \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T14:10:02.827287Z",
     "start_time": "2018-06-06T14:09:11.121575Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\envs\\TF_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " windows  Windows\n",
      "Tensorflow Version: 1.6.0   Keras Version : 2.1.4 \n",
      " Initialize config object - super\n",
      "(56, 56)\n",
      ">>> Initialize model WITHOUT MASKING LAYERS!!!!\n",
      "    set_log_dir: Checkpoint path set to : E:\\models\\mrcnn_oldshape_train_logs\\shapes20180606T1609\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "    set_log_dir: self.epoch set to 0 \n",
      "\n",
      ">>> Resnet Graph \n",
      "     Input_image shape : (?, 128, 128, 3)\n",
      "     After ZeroPadding2D  : (?, 134, 134, 3) (?, 134, 134, 3)\n",
      "     After Conv2D padding : (?, 64, 64, 64) (?, 64, 64, 64)\n",
      "     After BatchNorm      : (?, 64, 64, 64) (?, 64, 64, 64)\n",
      "     After MaxPooling2D   : (?, 32, 32, 64) (?, 32, 32, 64)\n",
      "\n",
      ">>> Feature Pyramid Network (FPN) Graph \n",
      "     FPN P2 shape : (None, 32, 32, 256)\n",
      "     FPN P3 shape : (None, 16, 16, 256)\n",
      "     FPN P4 shape : (None, 8, 8, 256)\n",
      "     FPN P5 shape : (None, 4, 4, 256)\n",
      "     FPN P6 shape : (None, 2, 2, 256)\n",
      "\n",
      ">>> RPN Layer \n",
      "     Input_feature_map shape : (?, ?, ?, 256)\n",
      "     anchors_per_location    : 3\n",
      "     depth                   : 256\n",
      "     Input_feature_map shape : (?, ?, ?, 256)\n",
      "     anchors_per_location    : 3\n",
      "     anchor_stride           : 1\n",
      "\n",
      ">>> RPN Outputs  <class 'list'>\n",
      "      rpn_class_logits/rpn_class_logits:0\n",
      "      rpn_class/rpn_class:0\n",
      "      rpn_bbox/rpn_bbox:0\n",
      "\n",
      ">>> Proposal Layer - generate  2000  proposals\n",
      "    Init complete. Size of anchors:  (4092, 4)\n",
      "     Scores :  (16, 4092)\n",
      "     Deltas :  (16, 4092, 4)\n",
      "     Anchors:  (16, 4092, 4)\n",
      "     Boxes shape / type after processing: \n",
      "     Output: Prposals shape :  (16, ?, ?) (16, None, None)\n",
      "\n",
      ">>> Detection Target Layer (Training Mode)\n",
      "    Detection Target Layer : call()  <class 'list'> 3\n",
      "     proposals.shape    : (16, ?, ?) (16, ?, ?) (None, 2000, 4)\n",
      "     gt_class_ids.shape : (?, ?) (?, ?) (None, None)\n",
      "     gt_bboxes.shape    : (?, ?, 4) (?, ?, 4) (None, None, 4)\n",
      "\n",
      "    Detection Target Layer : return  <class 'list'> 4\n",
      "     output 0  shape (16, ?, ?)  type <class 'tensorflow.python.framework.ops.Tensor'> \n",
      "     output 1  shape (16, ?)  type <class 'tensorflow.python.framework.ops.Tensor'> \n",
      "     output 2  shape (16, ?, ?)  type <class 'tensorflow.python.framework.ops.Tensor'> \n",
      "     output 3  shape (16, ?, ?)  type <class 'tensorflow.python.framework.ops.Tensor'> \n",
      "\n",
      ">>> FPN Classifier Graph \n",
      "     rois shape          : (16, ?, ?)\n",
      "     No of feature_maps  : 4\n",
      "        feature_maps shape  : (?, 32, 32, 256)\n",
      "        feature_maps shape  : (?, 16, 16, 256)\n",
      "        feature_maps shape  : (?, 8, 8, 256)\n",
      "        feature_maps shape  : (?, 4, 4, 256)\n",
      "     input_shape         : [128 128   3]\n",
      "     pool_size           : 7\n",
      "   > PyramidRoI Alignment Layer Call()  5\n",
      "     boxes.shape    : (None, 32, 4)\n",
      "     roi_align_classifier output shape is :  (1, ?, 7, 7, 256) (1, ?, 7, 7, 256)\n",
      "     mrcnn_class_conv1    output shape is :  (?, 32, 1, 1, 1024)\n",
      "     mrcnn_class_bn1      output shape is :  (?, 32, 1, 1, 1024)\n",
      "     mrcnn_class_relu1    output shape is :  (?, 32, 1, 1, 1024)\n",
      "     mrcnn_class_conv2 output shape is :  (?, 32, 1, 1, 1024)\n",
      "     mrcnn_class_bn2      output shape is :  (?, 32, 1, 1, 1024)\n",
      "     mrcnn_class_relu2    output shape is :  (?, 32, 1, 1, 1024)\n",
      "     pool_squeeze(Shared) output shape is :  (?, 32, 1024)\n",
      "     mrcnn_class_logits   output shape is :  (?, 32, 4)\n",
      "     mrcnn_class_probs    output shape is :  (?, 32, 4)\n",
      "   mrcnn_bbox_fc        output shape is :  (?, 32, 16)\n",
      "   mrcnn_bbox           output shape is :  (?, 32, 4, 4)\n",
      "\n",
      ">>> CHM Layer  \n",
      "   > CHMLayer Call()  5\n",
      "     mrcnn_class.shape    : (?, 32, 4) (None, 32, 4)\n",
      "     mrcnn_bbox.shape     : (?, 32, 4, 4) (None, 32, 4, 4)\n",
      "     output_rois.shape    : (16, ?, ?) (None, 32, 4)\n",
      "     tgt_class_ids.shape  : (16, ?) (None, 32)\n",
      "     gt_bboxes.shape      : (16, ?, ?) (None, 32, 4)\n",
      " config image shape:  [128 128   3] h: 128 w: 128\n",
      "\n",
      "  > build_predictions()\n",
      "    num_rois          :  32\n",
      "    mrcnn_class shape :  Tensor(\"cntxt_layer/Shape:0\", shape=(3,), dtype=int32) (None, 32, 4)\n",
      "    mrcnn_bbox.shape  :  Tensor(\"cntxt_layer/Shape_1:0\", shape=(4,), dtype=int32) (None, 32, 4, 4) (?, 32, 4, 4)\n",
      "    input_rois.shape :  Tensor(\"cntxt_layer/Shape_2:0\", shape=(3,), dtype=int32) (16, None, 4)\n",
      "    pred_array        (16, 32, 6)\n",
      "scatter_ind <class 'tensorflow.python.framework.ops.Tensor'> shape (16, 32, 3)\n",
      "    pred_scatter shape is  (16, 4, 32, 6)\n",
      "(16, 4, 32)\n",
      "\n",
      "\n",
      "  > BUILD_GROUND TRUTH_TF()\n",
      "\n",
      "    num_rois           :  32 (building  gt_tensor )\n",
      "    gt_class_ids shape :  (16, ?)\n",
      "    gt_bboxes.shape    :  (16, ?, 4)\n",
      "    gt_classes_exp shape  (16, ?, 1)\n",
      "    gt_scores_exp shape  (16, ?, 1)\n",
      "    gt_array shape : (16, 32, 7) (16, 32, 7)\n",
      "     gt_tensor final shape  :  (16, 4, 32, ?)\n",
      "\n",
      " \n",
      "  > NEW build_heatmap() for  ['pred_heatmap']\n",
      "    orignal in_tensor shape :  (16, 4, 32, 6)\n",
      "    num of bboxes per class is :  32\n",
      "    pt2_sum shape  (16, 4, 32)\n",
      "    dense shape  (?, 6)\n",
      "    X/Y shapes : (128, 128) (128, 128)\n",
      "    Ones:     (?, 1, 1)\n",
      "    ones_exp * X (?, 1, 1) * (128, 128) =  (?, 128, 128)\n",
      "    ones_exp * Y (?, 1, 1) * (128, 128) =  (?, 128, 128)\n",
      "    before transpse  (?, 128, 128, 2)\n",
      "    after transpose  (128, 128, ?, 2)\n",
      "     Prob_grid shape before tanspose:  (128, 128, ?)\n",
      "     Prob_grid shape after tanspose:  (?, 128, 128)\n",
      "    >> input to MVN.PROB: pos_grid (meshgrid) shape:  (128, 128, ?, 2)\n",
      "    << output probabilities shape: (?, 128, 128)\n",
      "\n",
      "    Scatter out the probability distributions based on class --------------\n",
      "    pt2_ind shape   :  (?, 3)\n",
      "    prob_grid shape :  (?, 128, 128)\n",
      "    gauss_scatt     :  (16, 4, 32, 128, 128)\n",
      "\n",
      "    Reduce sum based on class ---------------------------------------------\n",
      "    gaussian_sum shape     :  (16, 4, 128, 128) Keras tensor  False\n",
      "WARNING:tensorflow:From D:\\Program Files\\Anaconda3\\envs\\TF_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3157: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "    gauss L2 norm   :  (16, 4, 128, 128)  Keras tensor  False\n",
      "\n",
      "    normalization ------------------------------------------------------\n",
      "    gauss norm   :  (16, 4, 128, 128)  Keras tensor  False\n",
      "    in_tensor                (16, 4, 32, 6)\n",
      "    in_tensorr_flattened is  (?, ?)\n",
      "    boxes shape              (?, ?)\n",
      "    Rois per image        :  32\n",
      "    heatmap original shape  :  (16, 4, 128, 128)\n",
      "    heatmap replicated      :  (16, 4, 32, 128, 128)\n",
      "    heatmap flattened       :  (2048, 128, 128)\n",
      "    in_tensor_flattened     :  (?, ?)\n",
      "    Scores shape            :  (2048, 3)\n",
      "    boxes_scores (rehspaed) :  (?, ?, ?, ?)\n",
      "    gauss_heatmap final shape :  (16, 128, 128, 4)  Keras tensor  False\n",
      "    gauss_scores  final shape :  (?, ?, ?, ?)  Keras tensor  False\n",
      "    complete\n",
      "\n",
      " \n",
      "  > NEW build_heatmap() for  ['gt_heatmap']\n",
      "    orignal in_tensor shape :  (16, 4, 32, ?)\n",
      "    num of bboxes per class is :  32\n",
      "    pt2_sum shape  (16, 4, 32)\n",
      "    dense shape  (?, ?)\n",
      "    X/Y shapes : (128, 128) (128, 128)\n",
      "    Ones:     (?, 1, 1)\n",
      "    ones_exp * X (?, 1, 1) * (128, 128) =  (?, 128, 128)\n",
      "    ones_exp * Y (?, 1, 1) * (128, 128) =  (?, 128, 128)\n",
      "    before transpse  (?, 128, 128, 2)\n",
      "    after transpose  (128, 128, ?, 2)\n",
      "     Prob_grid shape before tanspose:  (128, 128, ?)\n",
      "     Prob_grid shape after tanspose:  (?, 128, 128)\n",
      "    >> input to MVN.PROB: pos_grid (meshgrid) shape:  (128, 128, ?, 2)\n",
      "    << output probabilities shape: (?, 128, 128)\n",
      "\n",
      "    Scatter out the probability distributions based on class --------------\n",
      "    pt2_ind shape   :  (?, 3)\n",
      "    prob_grid shape :  (?, 128, 128)\n",
      "    gauss_scatt     :  (16, 4, 32, 128, 128)\n",
      "\n",
      "    Reduce sum based on class ---------------------------------------------\n",
      "    gaussian_sum shape     :  (16, 4, 128, 128) Keras tensor  False\n",
      "    gauss L2 norm   :  (16, 4, 128, 128)  Keras tensor  False\n",
      "\n",
      "    normalization ------------------------------------------------------\n",
      "    gauss norm   :  (16, 4, 128, 128)  Keras tensor  False\n",
      "    in_tensor                (16, 4, 32, ?)\n",
      "    in_tensorr_flattened is  (?, ?)\n",
      "    boxes shape              (?, ?)\n",
      "    Rois per image        :  32\n",
      "    heatmap original shape  :  (16, 4, 128, 128)\n",
      "    heatmap replicated      :  (16, 4, 32, 128, 128)\n",
      "    heatmap flattened       :  (2048, 128, 128)\n",
      "    in_tensor_flattened     :  (?, ?)\n",
      "    Scores shape            :  (2048, 3)\n",
      "    boxes_scores (rehspaed) :  (?, ?, ?, ?)\n",
      "    gauss_heatmap final shape :  (16, 128, 128, 4)  Keras tensor  False\n",
      "    gauss_scores  final shape :  (?, ?, ?, ?)  Keras tensor  False\n",
      "    complete\n",
      "     pred_cls_cnt shape :  (16, 4) Keras tensor  True\n",
      "     gt_cls_cnt shape   :  (16, 4) Keras tensor  True\n",
      "     pred_heatmap_norm  :  (16, 128, 128, 4) Keras tensor  False\n",
      "     pred_heatmap_scores:  (?, ?, ?, ?) Keras tensor  False\n",
      "     gt_heatmap_norm    :  (16, 128, 128, 4) Keras tensor  False\n",
      "     gt_heatmap_scores  :  (?, ?, ?, ?) Keras tensor  False\n",
      "     complete\n",
      "<<<  shape of pred_heatmap   :  (16, 128, 128, 4)  Keras tensor  True\n",
      "<<<  shape of gt_heatmap     :  (16, 128, 128, 4)  Keras tensor  True\n",
      "\n",
      "\n",
      "---------------------------------------------------\n",
      "    Adding  FCN layers\n",
      "---------------------------------------------------\n",
      "\n",
      ">>> FCN Layer \n",
      "     feature map shape is  (16, 128, 128, 4)\n",
      "     height : 128 width : 128 classes : 4\n",
      "     image_data_format:  channels_last\n",
      "     rois_per_class   :  channels_last\n",
      "   FCN Block 11 shape is :  (16, 128, 128, 64)\n",
      "   FCN Block 12 shape is :  (16, 128, 128, 64)\n",
      "   FCN Block 13 shape is :  (16, 64, 64, 64)\n",
      "   FCN Block 21 shape is :  (16, 64, 64, 128)\n",
      "   FCN Block 22 shape is :  (16, 64, 64, 128)\n",
      "   FCN Block 23 (Max pooling) shape is :  (16, 32, 32, 128)\n",
      "   FCN Block 31 shape is :  (16, 32, 32, 256)\n",
      "   FCN Block 32 shape is :  (16, 32, 32, 256)\n",
      "   FCN Block 33 shape is :  (16, 32, 32, 256)\n",
      "   FCN Block 34 (Max pooling) shape is :  (16, 16, 16, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FCN fully connected 1 (fcn_fc1) shape is :  (16, 16, 16, 1024)\n",
      "   FCN fully connected 2 (fcn_fc2) shape is :  (16, 16, 16, 1024)\n",
      "   FCN final conv2d (fcn_classify) shape is :  (16, 16, 16, 4)  keras_tensor  True\n",
      "   h_factor :  8.0 w_factor :  8.0\n",
      "\n",
      ">>> BilinearUpSampling2D layer\n",
      "     data_format :  channels_last\n",
      "     size        :  (8.0, 8.0)\n",
      "     target_size :  None\n",
      "     input_spec  :  [InputSpec(ndim=4)]\n",
      "     call resize_images_bilinear with size:  (8.0, 8.0)\n",
      "     CHANNELS LAST: X:  (16, 16, 16, 4)  KB.int_shape() :  (None, 16, 16, 4)\n",
      "     target_height   :  None  target_width  :  None\n",
      "     new_shape (2):  (2,) (2,)\n",
      "     new_shape (3):  (2,) (2,)\n",
      "     X after image.resize_bilinear:  (16, ?, ?, 4)\n",
      "     Dimensions of X after set_shape() :  (16, 128, 128, 4)\n",
      "     BilinearUpSampling2D. compute_output_shape()\n",
      "     Bilinear output shape is: None , 128 , 128 , 4\n",
      "   FCN Bilinear upsmapling layer  shape is :  (16, 128, 128, 4)  Keras tensor  True\n",
      "\n",
      "\n",
      "    L2 normalization ------------------------------------------------------\n",
      "\n",
      "    normalization ------------------------------------------------------\n",
      "     size of reduce max is  (16, 1, 1, 4)\n",
      "     size of y is :  (16, 128, 128, 4)\n",
      "     size of reduce max is  (?, 1, 1, 4)\n",
      "     size of y is :  (?, 128, 128, 4)\n",
      "    fcn_heatmap       :  (16, 128, 128, 4)  Keras tensor  True\n",
      "    fcn_heatmap_norm  :  (16, 128, 128, 4)  Keras tensor  True\n",
      "    fcn_heatmap_L2norm:  (16, 128, 128, 4)  Keras tensor  True\n",
      "   fcn_heatmap      :  (None, 128, 128, 4)  Keras tensor  True\n",
      "   fcn_heatmap_norm :  (None, 128, 128, 4)  Keras tensor  True\n",
      "\n",
      ">>> FCN Scoring Layer \n",
      "   > FCNScoreLayer Call()  2\n",
      "     fcn_heatmap.shape    : (16, 128, 128, 4) (None, 128, 128, 4)\n",
      "      chm_scores.shape    : (?, ?, ?, ?) (None, 4, 32, 11)\n",
      "\n",
      " \n",
      "  > NEW build_heatmap() for  <mrcnn.shapes.ShapesConfig object at 0x0000022AC67A8358>\n",
      "    orignal in_heatmap shape :  (16, 128, 128, 4)\n",
      "    num of bboxes per class is :  32\n",
      "    Rois per image  :  32\n",
      "    heatmap original shape   :  (16, 128, 128, 4)\n",
      "    heatmap transposed shape : (16, 4, 128, 128)\n",
      "    heatmap tiled            :  (16, 4, 32, 128, 128)\n",
      "    fcn_scores  final shape :  (?, ?, ?, ?)  Keras tensor  False\n",
      "    complete\n",
      "\n",
      "    Output build_fcn_score \n",
      "     pred_heatmap_norm  :  (?, ?, ?, ?) Keras tensor  False\n",
      "     complete\n",
      "\n",
      ">>> fcn_norm_loss_graph \n",
      "    target_scores shape : (?, ?, ?)\n",
      "    pred_scores   shape : (?, ?, ?)\n",
      "    target_scores1 shape : (?, 1) (None, 1)\n",
      "    pred_scores1  shape : (?, 1)\n",
      "    loss type is : <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "\n",
      ">>> fcn_norm_loss_graph \n",
      "    target_scores shape : (?, 4, 100)\n",
      "    pred_scores   shape : (?, 4, 32)\n",
      "    target_scores1 shape : (?, 1) (None, 1)\n",
      "    pred_scores1  shape : (?, 1)\n",
      "    loss type is : <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "\n",
      "\n",
      "---------------------------------------------------\n",
      "    building Loss Functions \n",
      "---------------------------------------------------\n",
      " target_class_ids  : True (None, 32)\n",
      "\n",
      ">>> rpn_bbox_loss_graph\n",
      "    rpn_match size : (?, ?)\n",
      "    rpn_bbox  size : (?, ?, 4)\n",
      "    tf default session:  None\n",
      "\n",
      ">>> rpn_bbox_loss_graph\n",
      "    rpn_match size : (?, ?)\n",
      "    rpn_bbox  size : (?, ?, 4)\n",
      "    tf default session:  None\n",
      "\n",
      ">>> mrcnn_class_loss_graph \n",
      "    target_class_ids  size : (16, ?)\n",
      "    pred_class_logits size : (?, 32, 4)\n",
      "    active_class_ids  size : (?, ?)\n",
      "\n",
      ">>> mrcnn_class_loss_graph \n",
      "    target_class_ids  size : (?, 32)\n",
      "    pred_class_logits size : (?, 32, 4)\n",
      "    active_class_ids  size : (?, ?)\n",
      "\n",
      ">>> mrcnn_bbox_loss_graph \n",
      "    target_class_ids  size : (16, ?)\n",
      "    pred_bbox size         : (?, 32, 4, 4)\n",
      "    target_bbox size       : (16, ?, ?)\n",
      "    reshpaed pred_bbox size         : (?, 4, 4)\n",
      "    reshaped target_bbox size       : (?, 4)\n",
      "    pred_bbox size         : (?, 4)\n",
      "    target_bbox size       : (?, 4)\n",
      "\n",
      ">>> mrcnn_bbox_loss_graph \n",
      "    target_class_ids  size : (?, 32)\n",
      "    pred_bbox size         : (?, 32, 4, 4)\n",
      "    target_bbox size       : (?, 32, 4)\n",
      "    reshpaed pred_bbox size         : (?, 4, 4)\n",
      "    reshaped target_bbox size       : (?, 4)\n",
      "    pred_bbox size         : (?, 4)\n",
      "    target_bbox size       : (?, 4)\n",
      "\n",
      " Keras Tensors?? \n",
      " output_rois : True\n",
      " pr_hm       : True\n",
      " gt_heatmap  : True\n",
      " ================================================================\n",
      " self.keras_model.losses :  0\n",
      "[]\n",
      " ================================================================\n",
      "\n",
      ">>> MODIFIED MaskRCNN build complete -- WITHOUT MASKING LAYERS!!!!\n",
      ">>> MODIFIED MaskRCNN initialization complete -- WITHOUT MASKING LAYERS!!!!\n",
      " COCO Model Path       :  E:\\models\\mask_rcnn_coco.h5\n",
      " Checkpoint folder Path:  E:\\models\\mrcnn_oldshape_train_logs\n",
      " Model Parent Path     :  E:\\models\n",
      " Resent Model Path     :  E:\\models\\resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "-----------------------------------------------\n",
      " Load model with init parm:  last\n",
      ">>> find_last checkpoint in :  E:\\models\\mrcnn_oldshape_train_logs\n",
      " find last chkpt : ('E:\\\\models\\\\mrcnn_oldshape_train_logs\\\\shapes20180606T1150', 'E:\\\\models\\\\mrcnn_oldshape_train_logs\\\\shapes20180606T1150\\\\mask_rcnn_shapes_0008.h5')\n",
      "-----------------------------------------------\n",
      ">>> find_last checkpoint in :  E:\\models\\mrcnn_oldshape_train_logs\n",
      ">>> load_weights()\n",
      "    load_weights: Loading weights from: E:\\models\\mrcnn_oldshape_train_logs\\shapes20180606T1150\\mask_rcnn_shapes_0008.h5\n",
      "    load_weights: Log directory set to : E:\\models\\mrcnn_oldshape_train_logs\\shapes20180606T1150\\mask_rcnn_shapes_0008.h5\n",
      "    set_log_dir: Checkpoint path set to : E:\\models\\mrcnn_oldshape_train_logs\\shapes20180606T1150\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "    set_log_dir: self.epoch set to 9 \n",
      "    Load weights complete :  E:\\models\\mrcnn_oldshape_train_logs\\shapes20180606T1150\\mask_rcnn_shapes_0008.h5\n",
      "Load weights complete E:\\models\\mrcnn_oldshape_train_logs\\shapes20180606T1150\\mask_rcnn_shapes_0008.h5\n",
      "\n",
      "Configuration Parameters:\n",
      "-------------------------\n",
      "BACKBONE_SHAPES                [[32 32]\n",
      " [16 16]\n",
      " [ 8  8]\n",
      " [ 4  4]\n",
      " [ 2  2]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     16\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "EPOCHS_TO_RUN                  0\n",
      "FCN_INPUT_SHAPE                [128 128]\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 16\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LAST_EPOCH_RAN                 0\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "ROI_PROPOSAL_AREA_THRESHOLD    0\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                4\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import tensorflow as tf\n",
    "import keras.backend as KB\n",
    "import numpy as np\n",
    "from mrcnn.datagen     import data_generator, load_image_gt\n",
    "from mrcnn.callbacks   import get_layer_output_1,get_layer_output_2\n",
    "from mrcnn.utils       import mask_string\n",
    "import mrcnn.visualize as visualize\n",
    "from mrcnn.prep_notebook import prep_oldshapes_train, load_model\n",
    "# model_file = \"E:\\Models\\mrcnn_oldshape_train_logs\\TrainMRCNN\\mask_rcnn_shapes_1418.h5\"\n",
    "model_file = \"E:\\Models\\mrcnn_oldshape_train_logs\\shapes20180606T1150\\mask_rcnn_shapes_0008.h5\"\n",
    "model, dataset_train, dataset_val, train_generator, val_generator, config = prep_oldshapes_train(init_with = 'last', FCN_layers = True, batch_sz = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T14:06:05.829087Z",
     "start_time": "2018-06-05T14:06:05.602689Z"
    }
   },
   "outputs": [],
   "source": [
    "model.layer_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T16:42:44.455860Z",
     "start_time": "2018-05-28T16:42:43.010024Z"
    }
   },
   "outputs": [],
   "source": [
    "from mrcnn.prep_notebook import load_model\n",
    "load_model(model, init_with='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "source": [
    "### Training FPN, RPN and MRCNN heads using  Keras.model.fit_generator()\n",
    "\n",
    "print(config.BATCH_SIZE)\n",
    "print(model.config.BATCH_SIZE)\n",
    "print(model.config.LEARNING_RATE)\n",
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T21:37:28.251199Z",
     "start_time": "2018-06-05T21:11:11.215070Z"
    },
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "\n",
    "# model.train(dataset_train, dataset_val, \n",
    "#             learning_rate=config.LEARNING_RATE, \n",
    "# #             epochs = 69,\n",
    "#             epochs_to_run =2, \n",
    "#             layers='heads')\n",
    "## Last run prior to FCN training was 3699, last checkpoint was 3892  ...start at 3899\n",
    "\n",
    "train_layers = [ 'mrcnn', 'fpn','rpn']\n",
    "loss_names   = [ \"rpn_class_loss\", \"rpn_bbox_loss\" , \"mrcnn_class_loss\", \"mrcnn_bbox_loss\"]\n",
    "model.epoch = 1233\n",
    "model.config.LEARNING_RATE = 1.0e-4\n",
    "model.config.STEPS_PER_EPOCH = 7\n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=model.config.LEARNING_RATE, \n",
    "            epochs_to_run =3000, \n",
    "#             epochs = 25,            \n",
    "#             batch_size = 0\n",
    "#             steps_per_epoch = 0 \n",
    "            layers = train_layers,\n",
    "            losses = loss_names,\n",
    "            min_LR = 1.0e-6,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train FCN head layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-06T19:22:15.629Z"
    },
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fcn']\n",
      "['(fcn\\\\_.*)']\n",
      "layers regex : (fcn\\_.*)\n",
      " 213  fcn_block1_conv1       (Conv2D              )   TRAIN \n",
      " 214  fcn_block1_conv2       (Conv2D              )   TRAIN \n",
      " 216  fcn_block2_conv1       (Conv2D              )   TRAIN \n",
      " 217  fcn_block2_conv2       (Conv2D              )   TRAIN \n",
      " 219  fcn_block3_conv1       (Conv2D              )   TRAIN \n",
      " 220  fcn_block3_conv2       (Conv2D              )   TRAIN \n",
      " 221  fcn_block3_conv3       (Conv2D              )   TRAIN \n",
      " 223  fcn_fc1                (Conv2D              )   TRAIN \n",
      " 225  fcn_fc2                (Conv2D              )   TRAIN \n",
      " 227  fcn_classify           (Conv2D              )   TRAIN \n",
      "\n",
      "\n",
      " Compile Model :\n",
      "----------------\n",
      "    losses        :  ['fcn_norm_loss']\n",
      "    learning rate :  0.0001\n",
      "    momentum      :  0.9\n",
      "\n",
      "\n",
      " Add losses:\n",
      "----------------\n",
      "    losses:  ['fcn_norm_loss']\n",
      "    keras_model.losses           : []\n",
      "    Loss: fcn_norm_loss  Related Layer is : fcn_norm_loss\n",
      "      >> Add add loss for  Tensor(\"fcn_norm_loss/fcn_norm_loss:0\", shape=(1, 1), dtype=float32)  to list of losses...\n",
      "    Keras model.losses : \n",
      "[<tf.Tensor 'Mean_4:0' shape=(1, 1) dtype=float32>]\n",
      "    keras_model._losses:\n",
      "[<tf.Tensor 'Mean_4:0' shape=(1, 1) dtype=float32>]\n",
      "    keras_model._per_input_losses:\n",
      "{None: [<tf.Tensor 'Mean_4:0' shape=(1, 1) dtype=float32>]}\n",
      "    Final list of keras_model.losses \n",
      "[<tf.Tensor 'Mean_4:0' shape=(1, 1) dtype=float32>]\n",
      " Length of Keras_Model.outputs: 27\n",
      "\n",
      " Add Metrics :\n",
      "--------------\n",
      " Initial Keras metric_names: ['loss']\n",
      "    Loss name : fcn_norm_loss  Related Layer is : fcn_norm_loss\n",
      "      >> Add metric  fcn_norm_loss  with metric tensor:  fcn_norm_loss/fcn_norm_loss:0  to list of metrics ...\n",
      " Final Keras metric_names:\n",
      "['loss', 'fcn_norm_loss']\n",
      "\n",
      "Starting at epoch 1063 of 1563 epochs. LR=0.0001\n",
      "\n",
      "Steps per epochs 8 \n",
      "Batch size       16 \n",
      "Checkpoint Path: E:\\models\\mrcnn_oldshape_train_logs\\shapes20180606T1150\\mask_rcnn_shapes_{epoch:04d}.h5 \n",
      "================= CALLING FIT GENERATOR ==================\n",
      "@@@@@@@@@@@@@@ Get SGD updates: \n",
      " loss :  Tensor(\"loss_2/add:0\", shape=(1, 1), dtype=float32)\n",
      " params: \n",
      "[   <tf.Variable 'fcn_block1_conv1/kernel:0' shape=(3, 3, 4, 64) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_block1_conv1/bias:0' shape=(64,) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_block1_conv2/bias:0' shape=(64,) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_block2_conv1/bias:0' shape=(128,) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_block2_conv2/bias:0' shape=(128,) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_block3_conv1/bias:0' shape=(256,) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_block3_conv2/bias:0' shape=(256,) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_block3_conv3/bias:0' shape=(256,) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_fc1/kernel:0' shape=(7, 7, 256, 1024) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_fc1/bias:0' shape=(1024,) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_fc2/kernel:0' shape=(1, 1, 1024, 1024) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_fc2/bias:0' shape=(1024,) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_classify/kernel:0' shape=(1, 1, 1024, 4) dtype=float32_ref>,\n",
      "    <tf.Variable 'fcn_classify/bias:0' shape=(4,) dtype=float32_ref>]\n",
      "    params:  <tf.Variable 'fcn_block1_conv1/kernel:0' shape=(3, 3, 4, 64) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond/Merge:0\", shape=(3, 3, 4, 64), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable:0' shape=(3, 3, 4, 64) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_block1_conv1/bias:0' shape=(64,) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_1/Merge:0\", shape=(64,), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_1:0' shape=(64,) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_2/Merge:0\", shape=(3, 3, 64, 64), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_2:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_block1_conv2/bias:0' shape=(64,) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_3/Merge:0\", shape=(64,), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_3:0' shape=(64,) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_4/Merge:0\", shape=(3, 3, 64, 128), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_4:0' shape=(3, 3, 64, 128) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_block2_conv1/bias:0' shape=(128,) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_5/Merge:0\", shape=(128,), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_5:0' shape=(128,) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_6/Merge:0\", shape=(3, 3, 128, 128), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_6:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_block2_conv2/bias:0' shape=(128,) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_7/Merge:0\", shape=(128,), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_7:0' shape=(128,) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_8/Merge:0\", shape=(3, 3, 128, 256), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_8:0' shape=(3, 3, 128, 256) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_block3_conv1/bias:0' shape=(256,) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_9/Merge:0\", shape=(256,), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_9:0' shape=(256,) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_10/Merge:0\", shape=(3, 3, 256, 256), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_10:0' shape=(3, 3, 256, 256) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_block3_conv2/bias:0' shape=(256,) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_11/Merge:0\", shape=(256,), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_11:0' shape=(256,) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_12/Merge:0\", shape=(3, 3, 256, 256), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_12:0' shape=(3, 3, 256, 256) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_block3_conv3/bias:0' shape=(256,) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_13/Merge:0\", shape=(256,), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_13:0' shape=(256,) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_fc1/kernel:0' shape=(7, 7, 256, 1024) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_14/Merge:0\", shape=(7, 7, 256, 1024), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_14:0' shape=(7, 7, 256, 1024) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_fc1/bias:0' shape=(1024,) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_15/Merge:0\", shape=(1024,), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_15:0' shape=(1024,) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_fc2/kernel:0' shape=(1, 1, 1024, 1024) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_16/Merge:0\", shape=(1, 1, 1024, 1024), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_16:0' shape=(1, 1, 1024, 1024) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_fc2/bias:0' shape=(1024,) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_17/Merge:0\", shape=(1024,), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_17:0' shape=(1024,) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_classify/kernel:0' shape=(1, 1, 1024, 4) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_18/Merge:0\", shape=(1, 1, 1024, 4), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_18:0' shape=(1, 1, 1024, 4) dtype=float32_ref>\n",
      "    params:  <tf.Variable 'fcn_classify/bias:0' shape=(4,) dtype=float32_ref>\n",
      " gradients:  Tensor(\"training_2/SGD/cond_19/Merge:0\", shape=(4,), dtype=float32)\n",
      "  moements:  <tf.Variable 'training_2/SGD/Variable_19:0' shape=(4,) dtype=float32_ref>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1064/1563\n",
      "8/8 [==============================] - 30s 4s/step - loss: 0.0273 - fcn_norm_loss: 0.0273 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01064: val_loss improved from inf to 0.02554073, saving model to E:\\models\\mrcnn_oldshape_train_logs\\shapes20180606T1150\\mask_rcnn_shapes_1064.h5\n",
      "Epoch 1065/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01065: val_loss did not improve\n",
      "Epoch 1066/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0255 - fcn_norm_loss: 0.0255 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01066: val_loss did not improve\n",
      "Epoch 1067/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0283 - fcn_norm_loss: 0.0283 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01067: val_loss improved from 0.02554073 to 0.02548689, saving model to E:\\models\\mrcnn_oldshape_train_logs\\shapes20180606T1150\\mask_rcnn_shapes_1067.h5\n",
      "Epoch 1068/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01068: val_loss did not improve\n",
      "Epoch 1069/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0276 - fcn_norm_loss: 0.0276 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01069: val_loss did not improve\n",
      "Epoch 1070/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0274 - fcn_norm_loss: 0.0274 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01070: val_loss did not improve\n",
      "Epoch 1071/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01071: val_loss did not improve\n",
      "Epoch 1072/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01072: val_loss did not improve\n",
      "Epoch 1073/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0273 - fcn_norm_loss: 0.0273 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01073: val_loss did not improve\n",
      "Epoch 1074/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0275 - fcn_norm_loss: 0.0275 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01074: val_loss did not improve\n",
      "Epoch 1075/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0261 - fcn_norm_loss: 0.0261 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01075: val_loss did not improve\n",
      "Epoch 1076/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0273 - fcn_norm_loss: 0.0273 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01076: val_loss did not improve\n",
      "Epoch 1077/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0266 - fcn_norm_loss: 0.0266 - val_loss: 0.0254 - val_fcn_norm_loss: 0.0254\n",
      "\n",
      "Epoch 01077: val_loss improved from 0.02548689 to 0.02541627, saving model to E:\\models\\mrcnn_oldshape_train_logs\\shapes20180606T1150\\mask_rcnn_shapes_1077.h5\n",
      "Epoch 1078/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0262 - fcn_norm_loss: 0.0262 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01078: val_loss did not improve\n",
      "Epoch 1079/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0266 - fcn_norm_loss: 0.0266 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01079: val_loss did not improve\n",
      "Epoch 1080/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01080: val_loss did not improve\n",
      "Epoch 1081/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0258 - fcn_norm_loss: 0.0258 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01081: val_loss did not improve\n",
      "Epoch 1082/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01082: val_loss did not improve\n",
      "Epoch 1083/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0266 - fcn_norm_loss: 0.0266 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01083: val_loss did not improve\n",
      "Epoch 1084/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0257 - fcn_norm_loss: 0.0257 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01084: val_loss did not improve\n",
      "Epoch 1085/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01085: val_loss did not improve\n",
      "Epoch 1086/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0276 - fcn_norm_loss: 0.0276 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01086: val_loss did not improve\n",
      "Epoch 1087/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01087: val_loss did not improve\n",
      "Epoch 1088/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01088: val_loss did not improve\n",
      "Epoch 1089/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0253 - fcn_norm_loss: 0.0253 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01089: val_loss did not improve\n",
      "Epoch 1090/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0251 - fcn_norm_loss: 0.0251 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01090: val_loss did not improve\n",
      "Epoch 1091/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0272 - fcn_norm_loss: 0.0272 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01091: val_loss did not improve\n",
      "Epoch 1092/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0264 - fcn_norm_loss: 0.0264 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01092: val_loss did not improve\n",
      "Epoch 1093/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0255 - fcn_norm_loss: 0.0255 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01093: val_loss did not improve\n",
      "Epoch 1094/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0274 - fcn_norm_loss: 0.0274 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01094: val_loss did not improve\n",
      "Epoch 1095/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0258 - fcn_norm_loss: 0.0258 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01095: val_loss did not improve\n",
      "Epoch 1096/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0288 - fcn_norm_loss: 0.0288 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01096: val_loss did not improve\n",
      "Epoch 1097/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01097: val_loss did not improve\n",
      "Epoch 1098/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0275 - fcn_norm_loss: 0.0275 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01098: val_loss did not improve\n",
      "Epoch 1099/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01099: val_loss did not improve\n",
      "Epoch 1100/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0259 - fcn_norm_loss: 0.0259 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01100: val_loss did not improve\n",
      "Epoch 1101/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01101: val_loss did not improve\n",
      "Epoch 1102/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0273 - fcn_norm_loss: 0.0273 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01102: val_loss did not improve\n",
      "Epoch 1103/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0264 - fcn_norm_loss: 0.0264 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01103: val_loss did not improve\n",
      "Epoch 1104/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0269 - fcn_norm_loss: 0.0269 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01104: val_loss did not improve\n",
      "Epoch 1105/1563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 22s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01105: val_loss did not improve\n",
      "Epoch 1106/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0277 - fcn_norm_loss: 0.0277 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01106: val_loss did not improve\n",
      "Epoch 1107/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0258 - fcn_norm_loss: 0.0258 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01107: val_loss did not improve\n",
      "Epoch 1108/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0284 - fcn_norm_loss: 0.0284 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01108: val_loss did not improve\n",
      "Epoch 1109/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0283 - fcn_norm_loss: 0.0283 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01109: val_loss did not improve\n",
      "Epoch 1110/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0281 - fcn_norm_loss: 0.0281 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01110: val_loss did not improve\n",
      "Epoch 1111/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0265 - fcn_norm_loss: 0.0265 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01111: val_loss did not improve\n",
      "Epoch 1112/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0264 - fcn_norm_loss: 0.0264 - val_loss: 0.0264 - val_fcn_norm_loss: 0.0264\n",
      "\n",
      "Epoch 01112: val_loss did not improve\n",
      "Epoch 1113/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0264 - fcn_norm_loss: 0.0264 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01113: val_loss did not improve\n",
      "Epoch 1114/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0274 - fcn_norm_loss: 0.0274 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01114: val_loss did not improve\n",
      "Epoch 1115/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0280 - fcn_norm_loss: 0.0280 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01115: val_loss did not improve\n",
      "Epoch 1116/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01116: val_loss did not improve\n",
      "Epoch 1117/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0275 - fcn_norm_loss: 0.0275 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01117: val_loss did not improve\n",
      "Epoch 1118/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0253 - fcn_norm_loss: 0.0253 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01118: val_loss did not improve\n",
      "Epoch 1119/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0261 - fcn_norm_loss: 0.0261 - val_loss: 0.0253 - val_fcn_norm_loss: 0.0253\n",
      "\n",
      "Epoch 01119: val_loss improved from 0.02541627 to 0.02533682, saving model to E:\\models\\mrcnn_oldshape_train_logs\\shapes20180606T1150\\mask_rcnn_shapes_1119.h5\n",
      "Epoch 1120/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0281 - fcn_norm_loss: 0.0281 - val_loss: 0.0254 - val_fcn_norm_loss: 0.0254\n",
      "\n",
      "Epoch 01120: val_loss did not improve\n",
      "Epoch 1121/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0284 - fcn_norm_loss: 0.0284 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01121: val_loss did not improve\n",
      "Epoch 1122/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0254 - val_fcn_norm_loss: 0.0254\n",
      "\n",
      "Epoch 01122: val_loss did not improve\n",
      "Epoch 1123/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0275 - fcn_norm_loss: 0.0275 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01123: val_loss did not improve\n",
      "Epoch 1124/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0250 - fcn_norm_loss: 0.0250 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01124: val_loss did not improve\n",
      "Epoch 1125/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0280 - fcn_norm_loss: 0.0280 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01125: val_loss did not improve\n",
      "Epoch 1126/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0262 - fcn_norm_loss: 0.0262 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01126: val_loss did not improve\n",
      "Epoch 1127/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01127: val_loss did not improve\n",
      "Epoch 1128/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0266 - fcn_norm_loss: 0.0266 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01128: val_loss did not improve\n",
      "Epoch 1129/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0278 - fcn_norm_loss: 0.0278 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01129: val_loss did not improve\n",
      "Epoch 1130/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0275 - fcn_norm_loss: 0.0275 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01130: val_loss did not improve\n",
      "Epoch 1131/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01131: val_loss did not improve\n",
      "Epoch 1132/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01132: val_loss did not improve\n",
      "Epoch 1133/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01133: val_loss did not improve\n",
      "Epoch 1134/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0260 - fcn_norm_loss: 0.0260 - val_loss: 0.0254 - val_fcn_norm_loss: 0.0254\n",
      "\n",
      "Epoch 01134: val_loss did not improve\n",
      "Epoch 1135/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0273 - fcn_norm_loss: 0.0273 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01135: val_loss did not improve\n",
      "Epoch 1136/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0275 - fcn_norm_loss: 0.0275 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01136: val_loss did not improve\n",
      "Epoch 1137/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0278 - fcn_norm_loss: 0.0278 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01137: val_loss did not improve\n",
      "Epoch 1138/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0260 - fcn_norm_loss: 0.0260 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01138: val_loss did not improve\n",
      "Epoch 1139/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0264 - fcn_norm_loss: 0.0264 - val_loss: 0.0254 - val_fcn_norm_loss: 0.0254\n",
      "\n",
      "Epoch 01139: val_loss did not improve\n",
      "Epoch 1140/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0276 - fcn_norm_loss: 0.0276 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01140: val_loss did not improve\n",
      "Epoch 1141/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0260 - fcn_norm_loss: 0.0260 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01141: val_loss did not improve\n",
      "Epoch 1142/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01142: val_loss did not improve\n",
      "Epoch 1143/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0264 - fcn_norm_loss: 0.0264 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01143: val_loss did not improve\n",
      "Epoch 1144/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0274 - fcn_norm_loss: 0.0274 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01144: val_loss did not improve\n",
      "Epoch 1145/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0257 - fcn_norm_loss: 0.0257 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01145: val_loss did not improve\n",
      "Epoch 1146/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0252 - fcn_norm_loss: 0.0252 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01146: val_loss did not improve\n",
      "Epoch 1147/1563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 25s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01147: val_loss did not improve\n",
      "Epoch 1148/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0266 - fcn_norm_loss: 0.0266 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01148: val_loss did not improve\n",
      "\n",
      "Epoch 01148: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
      "Epoch 1149/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0274 - fcn_norm_loss: 0.0274 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01149: val_loss did not improve\n",
      "Epoch 1150/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0288 - fcn_norm_loss: 0.0288 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01150: val_loss did not improve\n",
      "Epoch 1151/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0254 - val_fcn_norm_loss: 0.0254\n",
      "\n",
      "Epoch 01151: val_loss did not improve\n",
      "Epoch 1152/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01152: val_loss did not improve\n",
      "Epoch 1153/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01153: val_loss did not improve\n",
      "Epoch 1154/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0266 - fcn_norm_loss: 0.0266 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01154: val_loss did not improve\n",
      "Epoch 1155/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0284 - fcn_norm_loss: 0.0284 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01155: val_loss did not improve\n",
      "Epoch 1156/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0256 - fcn_norm_loss: 0.0256 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01156: val_loss did not improve\n",
      "Epoch 1157/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0278 - fcn_norm_loss: 0.0278 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01157: val_loss did not improve\n",
      "Epoch 1158/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0276 - fcn_norm_loss: 0.0276 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01158: val_loss did not improve\n",
      "Epoch 1159/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0272 - fcn_norm_loss: 0.0272 - val_loss: 0.0253 - val_fcn_norm_loss: 0.0253\n",
      "\n",
      "Epoch 01159: val_loss improved from 0.02533682 to 0.02529729, saving model to E:\\models\\mrcnn_oldshape_train_logs\\shapes20180606T1150\\mask_rcnn_shapes_1159.h5\n",
      "Epoch 1160/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01160: val_loss did not improve\n",
      "Epoch 1161/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0272 - fcn_norm_loss: 0.0272 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01161: val_loss did not improve\n",
      "Epoch 1162/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01162: val_loss did not improve\n",
      "Epoch 1163/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0276 - fcn_norm_loss: 0.0276 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01163: val_loss did not improve\n",
      "Epoch 1164/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0269 - fcn_norm_loss: 0.0269 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01164: val_loss did not improve\n",
      "Epoch 1165/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0277 - fcn_norm_loss: 0.0277 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01165: val_loss did not improve\n",
      "Epoch 1166/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0276 - fcn_norm_loss: 0.0276 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01166: val_loss did not improve\n",
      "Epoch 1167/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0274 - fcn_norm_loss: 0.0274 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01167: val_loss did not improve\n",
      "Epoch 1168/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0283 - fcn_norm_loss: 0.0283 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01168: val_loss did not improve\n",
      "Epoch 1169/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0281 - fcn_norm_loss: 0.0281 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01169: val_loss did not improve\n",
      "Epoch 1170/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0252 - fcn_norm_loss: 0.0252 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01170: val_loss did not improve\n",
      "Epoch 1171/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0275 - fcn_norm_loss: 0.0275 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01171: val_loss did not improve\n",
      "Epoch 1172/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0262 - val_fcn_norm_loss: 0.0262\n",
      "\n",
      "Epoch 01172: val_loss did not improve\n",
      "Epoch 1173/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0259 - fcn_norm_loss: 0.0259 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01173: val_loss did not improve\n",
      "Epoch 1174/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0269 - fcn_norm_loss: 0.0269 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01174: val_loss did not improve\n",
      "Epoch 1175/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0266 - fcn_norm_loss: 0.0266 - val_loss: 0.0262 - val_fcn_norm_loss: 0.0262\n",
      "\n",
      "Epoch 01175: val_loss did not improve\n",
      "Epoch 1176/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0249 - fcn_norm_loss: 0.0249 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01176: val_loss did not improve\n",
      "Epoch 1177/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0273 - fcn_norm_loss: 0.0273 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01177: val_loss did not improve\n",
      "Epoch 1178/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0265 - fcn_norm_loss: 0.0265 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01178: val_loss did not improve\n",
      "Epoch 1179/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0257 - fcn_norm_loss: 0.0257 - val_loss: 0.0253 - val_fcn_norm_loss: 0.0253\n",
      "\n",
      "Epoch 01179: val_loss did not improve\n",
      "Epoch 1180/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0254 - fcn_norm_loss: 0.0254 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01180: val_loss did not improve\n",
      "Epoch 1181/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0248 - fcn_norm_loss: 0.0248 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01181: val_loss did not improve\n",
      "Epoch 1182/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0277 - fcn_norm_loss: 0.0277 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01182: val_loss did not improve\n",
      "Epoch 1183/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0269 - fcn_norm_loss: 0.0269 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01183: val_loss did not improve\n",
      "Epoch 1184/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0264 - fcn_norm_loss: 0.0264 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01184: val_loss did not improve\n",
      "Epoch 1185/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01185: val_loss did not improve\n",
      "Epoch 1186/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0279 - fcn_norm_loss: 0.0279 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01186: val_loss did not improve\n",
      "Epoch 1187/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0276 - fcn_norm_loss: 0.0276 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01187: val_loss did not improve\n",
      "Epoch 1188/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0280 - fcn_norm_loss: 0.0280 - val_loss: 0.0254 - val_fcn_norm_loss: 0.0254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01188: val_loss did not improve\n",
      "Epoch 1189/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01189: val_loss did not improve\n",
      "Epoch 1190/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0269 - fcn_norm_loss: 0.0269 - val_loss: 0.0262 - val_fcn_norm_loss: 0.0262\n",
      "\n",
      "Epoch 01190: val_loss did not improve\n",
      "Epoch 1191/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0254 - val_fcn_norm_loss: 0.0254\n",
      "\n",
      "Epoch 01191: val_loss did not improve\n",
      "Epoch 1192/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0260 - fcn_norm_loss: 0.0260 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01192: val_loss did not improve\n",
      "Epoch 1193/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0276 - fcn_norm_loss: 0.0276 - val_loss: 0.0262 - val_fcn_norm_loss: 0.0262\n",
      "\n",
      "Epoch 01193: val_loss did not improve\n",
      "Epoch 1194/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01194: val_loss did not improve\n",
      "Epoch 1195/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0253 - val_fcn_norm_loss: 0.0253\n",
      "\n",
      "Epoch 01195: val_loss did not improve\n",
      "Epoch 1196/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0250 - fcn_norm_loss: 0.0250 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01196: val_loss did not improve\n",
      "Epoch 1197/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01197: val_loss did not improve\n",
      "Epoch 1198/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0266 - fcn_norm_loss: 0.0266 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01198: val_loss did not improve\n",
      "Epoch 1199/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0269 - fcn_norm_loss: 0.0269 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01199: val_loss did not improve\n",
      "Epoch 1200/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0276 - fcn_norm_loss: 0.0276 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01200: val_loss did not improve\n",
      "Epoch 1201/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0259 - fcn_norm_loss: 0.0259 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01201: val_loss did not improve\n",
      "Epoch 1202/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01202: val_loss did not improve\n",
      "Epoch 1203/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01203: val_loss did not improve\n",
      "Epoch 1204/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01204: val_loss did not improve\n",
      "Epoch 1205/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0274 - fcn_norm_loss: 0.0274 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01205: val_loss did not improve\n",
      "Epoch 1206/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0259 - fcn_norm_loss: 0.0259 - val_loss: 0.0263 - val_fcn_norm_loss: 0.0263\n",
      "\n",
      "Epoch 01206: val_loss did not improve\n",
      "Epoch 1207/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0264 - fcn_norm_loss: 0.0264 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01207: val_loss did not improve\n",
      "Epoch 1208/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0264 - fcn_norm_loss: 0.0264 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01208: val_loss did not improve\n",
      "Epoch 1209/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0291 - fcn_norm_loss: 0.0291 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01209: val_loss did not improve\n",
      "Epoch 1210/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01210: val_loss did not improve\n",
      "Epoch 1211/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01211: val_loss did not improve\n",
      "Epoch 1212/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01212: val_loss did not improve\n",
      "Epoch 1213/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0269 - fcn_norm_loss: 0.0269 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01213: val_loss did not improve\n",
      "Epoch 1214/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0263 - val_fcn_norm_loss: 0.0263\n",
      "\n",
      "Epoch 01214: val_loss did not improve\n",
      "Epoch 1215/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0261 - fcn_norm_loss: 0.0261 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01215: val_loss did not improve\n",
      "Epoch 1216/1563\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.0277 - fcn_norm_loss: 0.0277 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01216: val_loss did not improve\n",
      "Epoch 1217/1563\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.0275 - fcn_norm_loss: 0.0275 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01217: val_loss did not improve\n",
      "Epoch 1218/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01218: val_loss did not improve\n",
      "Epoch 1219/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0273 - fcn_norm_loss: 0.0273 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01219: val_loss did not improve\n",
      "Epoch 1220/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0280 - fcn_norm_loss: 0.0280 - val_loss: 0.0264 - val_fcn_norm_loss: 0.0264\n",
      "\n",
      "Epoch 01220: val_loss did not improve\n",
      "Epoch 1221/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01221: val_loss did not improve\n",
      "Epoch 1222/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0269 - fcn_norm_loss: 0.0269 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01222: val_loss did not improve\n",
      "Epoch 1223/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0266 - fcn_norm_loss: 0.0266 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01223: val_loss did not improve\n",
      "Epoch 1224/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0260 - fcn_norm_loss: 0.0260 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01224: val_loss did not improve\n",
      "Epoch 1225/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0259 - fcn_norm_loss: 0.0259 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01225: val_loss did not improve\n",
      "Epoch 1226/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0273 - fcn_norm_loss: 0.0273 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01226: val_loss did not improve\n",
      "Epoch 1227/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01227: val_loss did not improve\n",
      "Epoch 1228/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01228: val_loss did not improve\n",
      "Epoch 1229/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0269 - fcn_norm_loss: 0.0269 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01229: val_loss did not improve\n",
      "Epoch 1230/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0263 - val_fcn_norm_loss: 0.0263\n",
      "\n",
      "Epoch 01230: val_loss did not improve\n",
      "Epoch 1231/1563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 23s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01231: val_loss did not improve\n",
      "Epoch 1232/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01232: val_loss did not improve\n",
      "Epoch 1233/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01233: val_loss did not improve\n",
      "Epoch 1234/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01234: val_loss did not improve\n",
      "Epoch 1235/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01235: val_loss did not improve\n",
      "Epoch 1236/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0284 - fcn_norm_loss: 0.0284 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01236: val_loss did not improve\n",
      "Epoch 1237/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0263 - val_fcn_norm_loss: 0.0263\n",
      "\n",
      "Epoch 01237: val_loss did not improve\n",
      "Epoch 1238/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0275 - fcn_norm_loss: 0.0275 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01238: val_loss did not improve\n",
      "Epoch 1239/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0273 - fcn_norm_loss: 0.0273 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01239: val_loss did not improve\n",
      "Epoch 1240/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01240: val_loss did not improve\n",
      "Epoch 1241/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0280 - fcn_norm_loss: 0.0280 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01241: val_loss did not improve\n",
      "Epoch 1242/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0264 - fcn_norm_loss: 0.0264 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01242: val_loss did not improve\n",
      "Epoch 1243/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0265 - fcn_norm_loss: 0.0265 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01243: val_loss did not improve\n",
      "Epoch 1244/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0275 - fcn_norm_loss: 0.0275 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01244: val_loss did not improve\n",
      "Epoch 1245/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0274 - fcn_norm_loss: 0.0274 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01245: val_loss did not improve\n",
      "Epoch 1246/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01246: val_loss did not improve\n",
      "Epoch 1247/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0274 - fcn_norm_loss: 0.0274 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01247: val_loss did not improve\n",
      "Epoch 1248/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0259 - fcn_norm_loss: 0.0259 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01248: val_loss did not improve\n",
      "Epoch 1249/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0265 - fcn_norm_loss: 0.0265 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01249: val_loss did not improve\n",
      "Epoch 1250/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0264 - fcn_norm_loss: 0.0264 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01250: val_loss did not improve\n",
      "Epoch 1251/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01251: val_loss did not improve\n",
      "Epoch 1252/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0258 - fcn_norm_loss: 0.0258 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01252: val_loss did not improve\n",
      "Epoch 1253/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01253: val_loss did not improve\n",
      "\n",
      "Epoch 01253: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
      "Epoch 1254/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01254: val_loss did not improve\n",
      "Epoch 1255/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01255: val_loss did not improve\n",
      "Epoch 1256/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01256: val_loss did not improve\n",
      "Epoch 1257/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0272 - fcn_norm_loss: 0.0272 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01257: val_loss did not improve\n",
      "Epoch 1258/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01258: val_loss did not improve\n",
      "Epoch 1259/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0280 - fcn_norm_loss: 0.0280 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01259: val_loss did not improve\n",
      "Epoch 1260/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0260 - fcn_norm_loss: 0.0260 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01260: val_loss did not improve\n",
      "Epoch 1261/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0257 - fcn_norm_loss: 0.0257 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01261: val_loss did not improve\n",
      "Epoch 1262/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0260 - fcn_norm_loss: 0.0260 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01262: val_loss did not improve\n",
      "Epoch 1263/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0266 - fcn_norm_loss: 0.0266 - val_loss: 0.0252 - val_fcn_norm_loss: 0.0252\n",
      "\n",
      "Epoch 01263: val_loss improved from 0.02529729 to 0.02521119, saving model to E:\\models\\mrcnn_oldshape_train_logs\\shapes20180606T1150\\mask_rcnn_shapes_1263.h5\n",
      "Epoch 1264/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0277 - fcn_norm_loss: 0.0277 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01264: val_loss did not improve\n",
      "Epoch 1265/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0277 - fcn_norm_loss: 0.0277 - val_loss: 0.0254 - val_fcn_norm_loss: 0.0254\n",
      "\n",
      "Epoch 01265: val_loss did not improve\n",
      "Epoch 1266/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01266: val_loss did not improve\n",
      "Epoch 1267/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0265 - fcn_norm_loss: 0.0265 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01267: val_loss did not improve\n",
      "Epoch 1268/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0276 - fcn_norm_loss: 0.0276 - val_loss: 0.0262 - val_fcn_norm_loss: 0.0262\n",
      "\n",
      "Epoch 01268: val_loss did not improve\n",
      "Epoch 1269/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01269: val_loss did not improve\n",
      "Epoch 1270/1563\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01270: val_loss did not improve\n",
      "Epoch 1271/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0269 - fcn_norm_loss: 0.0269 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01271: val_loss did not improve\n",
      "Epoch 1272/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01272: val_loss did not improve\n",
      "Epoch 1273/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0265 - fcn_norm_loss: 0.0265 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01273: val_loss did not improve\n",
      "Epoch 1274/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0274 - fcn_norm_loss: 0.0274 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01274: val_loss did not improve\n",
      "Epoch 1275/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0282 - fcn_norm_loss: 0.0282 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01275: val_loss did not improve\n",
      "Epoch 1276/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01276: val_loss did not improve\n",
      "Epoch 1277/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0275 - fcn_norm_loss: 0.0275 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01277: val_loss did not improve\n",
      "Epoch 1278/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0262 - fcn_norm_loss: 0.0262 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01278: val_loss did not improve\n",
      "Epoch 1279/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0276 - fcn_norm_loss: 0.0276 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01279: val_loss did not improve\n",
      "Epoch 1280/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0262 - val_fcn_norm_loss: 0.0262\n",
      "\n",
      "Epoch 01280: val_loss did not improve\n",
      "Epoch 1281/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0262 - val_fcn_norm_loss: 0.0262\n",
      "\n",
      "Epoch 01281: val_loss did not improve\n",
      "Epoch 1282/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0257 - fcn_norm_loss: 0.0257 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01282: val_loss did not improve\n",
      "Epoch 1283/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0272 - fcn_norm_loss: 0.0272 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01283: val_loss did not improve\n",
      "Epoch 1284/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0272 - fcn_norm_loss: 0.0272 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01284: val_loss did not improve\n",
      "Epoch 1285/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0272 - fcn_norm_loss: 0.0272 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01285: val_loss did not improve\n",
      "Epoch 1286/1563\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0260 - fcn_norm_loss: 0.0260 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01286: val_loss did not improve\n",
      "Epoch 1287/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0252 - fcn_norm_loss: 0.0252 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01287: val_loss did not improve\n",
      "Epoch 1288/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0280 - fcn_norm_loss: 0.0280 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01288: val_loss did not improve\n",
      "Epoch 1289/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0275 - fcn_norm_loss: 0.0275 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01289: val_loss did not improve\n",
      "Epoch 1290/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0264 - fcn_norm_loss: 0.0264 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01290: val_loss did not improve\n",
      "Epoch 1291/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0282 - fcn_norm_loss: 0.0282 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01291: val_loss did not improve\n",
      "Epoch 1292/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0273 - fcn_norm_loss: 0.0273 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01292: val_loss did not improve\n",
      "Epoch 1293/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01293: val_loss did not improve\n",
      "Epoch 1294/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01294: val_loss did not improve\n",
      "Epoch 1295/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0247 - fcn_norm_loss: 0.0247 - val_loss: 0.0262 - val_fcn_norm_loss: 0.0262\n",
      "\n",
      "Epoch 01295: val_loss did not improve\n",
      "Epoch 1296/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0262 - fcn_norm_loss: 0.0262 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01296: val_loss did not improve\n",
      "Epoch 1297/1563\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0260 - fcn_norm_loss: 0.0260 - val_loss: 0.0254 - val_fcn_norm_loss: 0.0254\n",
      "\n",
      "Epoch 01297: val_loss did not improve\n",
      "Epoch 1298/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0264 - fcn_norm_loss: 0.0264 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01298: val_loss did not improve\n",
      "Epoch 1299/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0269 - fcn_norm_loss: 0.0269 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01299: val_loss did not improve\n",
      "Epoch 1300/1563\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.0257 - fcn_norm_loss: 0.0257 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01300: val_loss did not improve\n",
      "Epoch 1301/1563\n",
      "8/8 [==============================] - 27s 3s/step - loss: 0.0275 - fcn_norm_loss: 0.0275 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01301: val_loss did not improve\n",
      "Epoch 1302/1563\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01302: val_loss did not improve\n",
      "Epoch 1303/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0286 - fcn_norm_loss: 0.0286 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01303: val_loss did not improve\n",
      "Epoch 1304/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0257 - fcn_norm_loss: 0.0257 - val_loss: 0.0253 - val_fcn_norm_loss: 0.0253\n",
      "\n",
      "Epoch 01304: val_loss did not improve\n",
      "Epoch 1305/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0258 - fcn_norm_loss: 0.0258 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01305: val_loss did not improve\n",
      "Epoch 1306/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0269 - fcn_norm_loss: 0.0269 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01306: val_loss did not improve\n",
      "Epoch 1307/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01307: val_loss did not improve\n",
      "Epoch 1308/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01308: val_loss did not improve\n",
      "Epoch 1309/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0282 - fcn_norm_loss: 0.0282 - val_loss: 0.0263 - val_fcn_norm_loss: 0.0263\n",
      "\n",
      "Epoch 01309: val_loss did not improve\n",
      "Epoch 1310/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0274 - fcn_norm_loss: 0.0274 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01310: val_loss did not improve\n",
      "Epoch 1311/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0272 - fcn_norm_loss: 0.0272 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01311: val_loss did not improve\n",
      "Epoch 1312/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0273 - fcn_norm_loss: 0.0273 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01312: val_loss did not improve\n",
      "Epoch 1313/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01313: val_loss did not improve\n",
      "Epoch 1314/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0278 - fcn_norm_loss: 0.0278 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01314: val_loss did not improve\n",
      "Epoch 1315/1563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 22s 3s/step - loss: 0.0272 - fcn_norm_loss: 0.0272 - val_loss: 0.0255 - val_fcn_norm_loss: 0.0255\n",
      "\n",
      "Epoch 01315: val_loss did not improve\n",
      "Epoch 1316/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0281 - fcn_norm_loss: 0.0281 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01316: val_loss did not improve\n",
      "Epoch 1317/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01317: val_loss did not improve\n",
      "Epoch 1318/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0250 - fcn_norm_loss: 0.0250 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01318: val_loss did not improve\n",
      "Epoch 1319/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0263 - fcn_norm_loss: 0.0263 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01319: val_loss did not improve\n",
      "Epoch 1320/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0274 - fcn_norm_loss: 0.0274 - val_loss: 0.0261 - val_fcn_norm_loss: 0.0261\n",
      "\n",
      "Epoch 01320: val_loss did not improve\n",
      "Epoch 1321/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0265 - fcn_norm_loss: 0.0265 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01321: val_loss did not improve\n",
      "Epoch 1322/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0276 - fcn_norm_loss: 0.0276 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01322: val_loss did not improve\n",
      "Epoch 1323/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01323: val_loss did not improve\n",
      "Epoch 1324/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0266 - fcn_norm_loss: 0.0266 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01324: val_loss did not improve\n",
      "Epoch 1325/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0273 - fcn_norm_loss: 0.0273 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01325: val_loss did not improve\n",
      "Epoch 1326/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0257 - val_fcn_norm_loss: 0.0257\n",
      "\n",
      "Epoch 01326: val_loss did not improve\n",
      "Epoch 1327/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0273 - fcn_norm_loss: 0.0273 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01327: val_loss did not improve\n",
      "Epoch 1328/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0271 - fcn_norm_loss: 0.0271 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01328: val_loss did not improve\n",
      "Epoch 1329/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0258 - fcn_norm_loss: 0.0258 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01329: val_loss did not improve\n",
      "Epoch 1330/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01330: val_loss did not improve\n",
      "Epoch 1331/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0267 - fcn_norm_loss: 0.0267 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01331: val_loss did not improve\n",
      "Epoch 1332/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0269 - fcn_norm_loss: 0.0269 - val_loss: 0.0254 - val_fcn_norm_loss: 0.0254\n",
      "\n",
      "Epoch 01332: val_loss did not improve\n",
      "Epoch 1333/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0279 - fcn_norm_loss: 0.0279 - val_loss: 0.0260 - val_fcn_norm_loss: 0.0260\n",
      "\n",
      "Epoch 01333: val_loss did not improve\n",
      "Epoch 1334/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0270 - fcn_norm_loss: 0.0270 - val_loss: 0.0256 - val_fcn_norm_loss: 0.0256\n",
      "\n",
      "Epoch 01334: val_loss did not improve\n",
      "Epoch 1335/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0266 - fcn_norm_loss: 0.0266 - val_loss: 0.0262 - val_fcn_norm_loss: 0.0262\n",
      "\n",
      "Epoch 01335: val_loss did not improve\n",
      "Epoch 1336/1563\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0268 - fcn_norm_loss: 0.0268 - val_loss: 0.0258 - val_fcn_norm_loss: 0.0258\n",
      "\n",
      "Epoch 01336: val_loss did not improve\n",
      "Epoch 1337/1563\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0276 - fcn_norm_loss: 0.0276 - val_loss: 0.0259 - val_fcn_norm_loss: 0.0259\n",
      "\n",
      "Epoch 01337: val_loss did not improve\n",
      "Epoch 1338/1563\n",
      "1/8 [==>...........................] - ETA: 17s - loss: 0.0290 - fcn_norm_loss: 0.0290"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "\n",
    "# model.train(dataset_train, dataset_val, \n",
    "#             learning_rate=config.LEARNING_RATE, \n",
    "# #             epochs = 69,\n",
    "#             epochs_to_run =2, \n",
    "#             layers='heads')\n",
    "\n",
    "## Last run prior to FCN training was 3699, last checkpoint was 3892\n",
    "\n",
    "train_layers = ['fcn']\n",
    "loss_names   = [  \"fcn_norm_loss\"]\n",
    "model.epoch = 1063\n",
    "model.config.LEARNING_RATE = 1.0e-4\n",
    "model.config.STEPS_PER_EPOCH = 8 \n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=model.config.LEARNING_RATE, \n",
    "            epochs_to_run = 500, \n",
    "#             epochs = 25,            \n",
    "#             batch_size = 6,\n",
    "#             steps_per_epoch = 0 \n",
    "            layers = train_layers,\n",
    "            losses = loss_names,\n",
    "            min_LR = 1.0e-9\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T18:17:32.353508Z",
     "start_time": "2018-05-20T18:17:32.121048Z"
    }
   },
   "outputs": [],
   "source": [
    "model.keras_model.losses\n",
    "print(model.keras_model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## - Training heads using train_in_batches ()\n",
    "\n",
    "We need to use this method for the time being as the fit generator does not have provide EASY access to the output in Keras call backs. By training in batches, we pass a batch through the network, pick up the generated RoI detections and bounding boxes and generate our semantic / gaussian tensors ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T15:03:53.709099Z",
     "start_time": "2018-04-28T15:02:36.185321Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.train_in_batches(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE/6, \n",
    "            epochs_to_run = 3,\n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Fine Tuning\n",
    "Fine tune all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=211,\n",
    "            layers=\"all\")\n",
    "\n",
    "# train_layers = ['fcn']\n",
    "# loss_names   = [  \"fcn_norm_loss\"]\n",
    "# model.epoch = 208\n",
    "# model.config.LEARNING_RATE = 1.0e-4\n",
    "# model.config.STEPS_PER_EPOCH = 8 \n",
    "\n",
    "# model.train(dataset_train, dataset_val, \n",
    "#             learning_rate=model.config.LEARNING_RATE, \n",
    "#             epochs_to_run = 500, \n",
    "# #             epochs = 25,            \n",
    "# #             batch_size = 6,\n",
    "# #             steps_per_epoch = 0 \n",
    "#             layers = train_layers,\n",
    "#             losses = loss_names,\n",
    "#             min_LR = 1.0e-7\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes_post_training.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T18:25:16.962148Z",
     "start_time": "2018-05-20T18:25:16.737938Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.keras_model.summary(line_length=132, positions=[0.30,0.75, .83, 1. ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T14:10:55.871863Z",
     "start_time": "2018-06-05T14:10:51.289152Z"
    },
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "train_batch_x, train_batch_y = next(train_generator)\n",
    "imgmeta_idx = model.keras_model.input_names.index('input_image_meta')\n",
    "img_meta    = train_batch_x[imgmeta_idx]\n",
    "\n",
    "for img_idx in range(config.BATCH_SIZE):\n",
    "    image_id = img_meta[img_idx,0]\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    print('Image id: ',image_id)\n",
    "    print('Image meta', img_meta[img_idx])\n",
    "    print('Classes (1: circle, 2: square, 3: triangle ): ',class_ids)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T14:10:58.357215Z",
     "start_time": "2018-06-05T14:10:58.128846Z"
    },
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "model.layer_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T14:11:29.002604Z",
     "start_time": "2018-06-05T14:11:21.360692Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_batch_x, train_batch_y = next(train_generator)\n",
    "\n",
    "model_output = get_layer_output_2(model.keras_model, train_batch_x, 1)\n",
    "# model_output = get_layer_output_1(model.keras_model, train_batch_x, [ 26], 1)\n",
    "\n",
    "print(len(model_output))\n",
    "\n",
    "# rpn_class_loss            = model_output[0]          # layer: 11   shape: (1, 1)\n",
    "# rpn_bbox_loss             = model_output[1]          # layer: 12   shape: (1, 1)\n",
    "# mrcnn_class_loss          = model_output[2]          # layer: 13   shape: (1, 1)\n",
    "# mrcnn_bbox_loss           = model_output[3]          # layer: 14   shape: (1, 1)\n",
    "# fcn_normalized_loss       = model_output[0]          # layer: 26   shape: (1, 1)\n",
    "\n",
    "# print(type(output_rois))\n",
    "for i in model_output:\n",
    "    print( i.shape)\n",
    "# print('FCN Normalized Loss is :', fcn_normalized_loss)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T18:40:52.258442Z",
     "start_time": "2018-05-20T18:40:52.031879Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_image      =  train_batch_x[0]\n",
    "input_image_meta =  train_batch_x[1]\n",
    "input_rpn_match  =  train_batch_x[2]\n",
    "input_rpn_bbox   =  train_batch_x[3]\n",
    "input_gt_class_ids = train_batch_x[4]\n",
    "input_gt_bboxes    = train_batch_x[5]\n",
    "input_gt_masks     = train_batch_x[6]\n",
    "print(' Input image shape is :', input_image.shape)\n",
    "h, w = input_image.shape[1], input_image.shape[2]      #  tf.shape(input_image)[1], tf.shape(input_image)[2]\n",
    "input_normlzd_gt_bboxes = tf.identity(input_gt_bboxes / [h,w,h,w])\n",
    "\n",
    "# gt_masks   =  train_batch_x[6]\n",
    "print(' input_rpn_match    ', input_rpn_match.shape)\n",
    "print(' input_rpn_bbox     ', input_rpn_bbox.shape)\n",
    "print(' input_gt_class_ids ', input_gt_class_ids.shape)\n",
    "print(' input_gt_bboxes    ', input_gt_bboxes.shape)\n",
    "print(' input_normlzd_gt_bboxes    ', input_normlzd_gt_bboxes.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python [conda env:TF_gpu]",
   "language": "python",
   "name": "conda-env-TF_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
